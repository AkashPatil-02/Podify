# Podify

A small app and API that converts long-form text or web articles into spoken podcast audio files.

Podify contains a Flutter client (mobile) and a Node.js server that:

- extracts text from a URL or accepts raw text
- (optionally) summarizes the text using a generative model
- converts the text to speech in chunks (Deepgram TTS by default)
- merges the generated audio into a single downloadable MP3

This README helps developers get the project running locally and explains the server API used by the Flutter client.

**Contents**

- What it does
- Why it’s useful
- Quick start (server & mobile)
- Environment variables
- API usage examples
- Development notes & where to get help

---

**What the project does**

Podify turns text or web pages into spoken podcast audio. The server can extract text from an article URL, summarize it with a generative model if requested, synthesize speech in chunks to respect TTS limits, and return a URL to the produced MP3.

**Why Podify is useful**

- Easily produce spoken versions of long-form content for listening on the go.
- Demonstrates a full-stack flow: web scraping, optional summarization, TTS, audio post-processing (FFmpeg), and a Flutter front-end.
- Useful as a base for accessibility, podcast generation, or content repurposing tools.

---

**Repository layout (important parts)**

- `server/` — Node.js API and TTS pipeline
  - `app.js` — Express server entrypoint
  - `routes/podcast.js` — endpoint to create podcasts (`POST /api/podcast`)
  - `utils/` — helpers: `extractText.js`, `summarize.js`, `tts.js`, `mergeAudio.js`
- `lib/` — Flutter app source (mobile client)

---

**Quick start — Server (Node.js)**

1. Install dependencies and create `.env` in `server/`:

```powershell
cd server
npm install
copy NUL .env
# Edit .env and add required keys (see next section)
```

2. Run server in development (nodemon):

```powershell
npm run dev
```

By default the server listens on `process.env.PORT` or `5000`.

Files generated by the server (audio) are served from the `audio/` directory via `/audio`.

**Quick start — Flutter app**

From repository root:

```bash
cd <repo-root>
flutter pub get
flutter run
```

Configure the mobile client to point to your running server (see `server.app.js` port or `podcast.js` audio URL). The client uses the server API to request podcast generation.

---

**Environment variables**

Create a `server/.env` file with the following keys (examples):

```env
# Deepgram API key used for TTS
deepgramApiKey=sk_xxx

# Google Gemini / Generative AI key used for summarization (optional)
GEMINI_KEY=ya29.xxxx

# Optional: PORT for server (default 5000)
PORT=5000
```

Notes:

- `deepgramApiKey` is required for the default TTS implementation in `server/utils/tts.js`.
- Summarization is optional; the server will use the full extracted text if summarization is disabled or returns less text than the original input.

---

**API — Podcast generation**

POST /api/podcast

Request JSON body:

```json
{
  "text": "<raw text to convert>",
  "url": "https://example.com/article", // optional, server will extract text
  "summarize": true, // optional, run summarization first
  "model": "<optional TTS voice model identifier>"
}
```

Response (success):

```json
{
  "status": "success",
  "audio_url": "http://<server>/audio/merged_12345.mp3",
  "chunks": 3,
  "transcript": "..."
}
```

Example curl (use your server address/port):

```bash
curl -X POST http://localhost:5000/api/podcast \
	-H "Content-Type: application/json" \
	-d '{"url":"https://example.com/article","summarize":false}'
```

Important: the server currently builds the returned `audio_url` from a hardcoded host/port in `routes/podcast.js`; update that URL to match your deployment or use a reverse proxy.

---

**Development notes & maintenance**

- TTS: `server/utils/tts.js` uses the Deepgram SDK by default — update or replace the TTS provider if desired.
- Audio merging: `server/utils/mergeAudio.js` uses FFmpeg (via `@ffmpeg-installer/ffmpeg` & `fluent-ffmpeg`) — ensure native FFmpeg is available in your environment.
- `server/utils/extractText.js` uses `cheerio` to extract body text from HTML — this is a simple approach and may not suit all sites.

If you plan to deploy to production:

- Replace hardcoded URLs in `routes/podcast.js` with `process.env` configuration.
- Secure secrets (do not commit `.env`), and consider rate limits / queueing for large TTS jobs.

---

**Where to get help & contribute**

- Report issues or feature requests via the repository Issues.
- For code contributions, open a Pull Request. See `docs/CONTRIBUTING.md` for guidance (create this file if you want to add project-specific rules).

**Maintainers**

- Repository owner: `AkashPatil-02` (see GitHub for contact and profile).

---

If you want, I can also:

- add a sample `.env.example` in `server/`
- update `routes/podcast.js` to build `audio_url` dynamically from `req.headers.host` or an `APP_URL` env var
- add a small test script that runs the chunking algorithm locally so you can tune `chunkSize`

Thank you — tell me which of the optional follow-ups you want and I will implement them.
